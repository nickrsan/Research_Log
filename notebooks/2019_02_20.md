## Eflows Clustering - 2019-02-20
Did an analysis of local huc assemblages versus cluster assemblages to understand
which hucs and clusters vary significantly based on our current clustering. This can help
inform how useful a cluster may be for managers (since it captures a large or small amount
of the variability within the cluster). Southern California and the northern Sierra did poorly, but smaller clusters in
the Klamath and Great Basin did well (at least in matching cluster assemblages to local assemblages). It's possible this
metric is just flawed, but it can help us understand how much additional variation is external to each HUC for management.
For clusters where, across the cluster, there is a low proportion of species in each huc that are in the cluster assemblage,
managers may have different ideas. But we might also want to try a different clustering method in these areas - it's possible
that this structure is inherent though and changing the method may not matter.

## 2019-02-21
I was curious if we could get a more uniform distribution of (number of species in HUC)/(number of species in cluster) by
doing one clustering run across the state (removing our regionalization/forced barriers). Given the way
the clustering algorithm tries to maximize cluster uniformity and differences between clusters, it seemed like it
should do a bit better. I ran a clustering run, and it did change the results, but it made almost everything south of San
Francisco into a single cluster, which would exacerbate the problem we were experiencing. This is similar to what we previously
saw with statewide clustering.

## 2019-02-22
Closing some old tabs and came across on-premises servers for DVCS systems - might need one for PISCES because the 
database itself is serverless, but we want it versioned. Other option would be to move the DB to postgres (since most
operations are now in an ORM, translation should be smoother) and version data there. Still, it'd be nice to dump SQL data
and back it up. Looked at HgLab, and it seemed good, but uses SQL Server Express and setup might be painful - I wanted to follow
up with devs about what it uses SQL Server Express for (to make sure I won't hit size limits - I imagine it's just for
HgLab info and not repos, but want to confirm). I also checked out BitBucket server, but it doesn't support Hg! Another option would be to
move to Git and use GitLFS instead. Would like the text to actually be versioned though. In searching, one of the other options
that could work well is RhodeCode - it has a free version that supports the features we'd need. https://rhodecode.com/pricing.
 also had this thread in UCD Slack about it https://ucdavis.slack.com/archives/C04LQB62B/p1545624834004900
 
## 2019-02-27
### PISCES hosting
I'm now thinking that I'll use a hybrid Git/Mercurial strategy for the PISCES repository. Each one will exclude the items
tracked by the other. Hg will track code, documentation, and code-related items and Git will track assets and data. It'd
likely be good to do a strategy where Git commits first, and then the required git hash can be added to the Hg commit.
This is a bit of a silly strategy, but here are my reasons. I'd like to keep Hg because:
1) I think it's easier to teach and get people started with (backed up by some of what I read)
2) It's way better for not accidentally destroying stuff
3) Most other people working on this repo are brand new to VCS/DVCS and
4) I don't want my migration to be construed as market share for Git - I see that Atlassian isn't supporting Hg in
Bitbucket Server because there's not enough market share, but how much of that is because lack of tooling. 
I need a system that supports largefiles, like GitLFS, but don't need it to be in Git. I have a lot of repos in Git,
but really like that this one is Hg

We could do this all in Git, but I like that this repo uses Mercurial. I'd be willing to have a tiny bit of extra overhead
as two repositories in order to use Hg for the core stuff. This still gives us DB downloads for people, but bypasses
the lack of a Mercurial hosting option we can use that doesn't require us to set up and maintain ingrastructure. Changing
PISCES to use a DB server is an option, but that still requires infrastructure, and limits the portability that is a major
selling point right now.

### Mantis/Convolution speed for Nitrates/NPSAT
Giorgos' initial compiled linux executable seemed to have some issues on the server, so we're considering alternatives.
I did some investigation of the `Numba` package for Python, a Just-In-Time (JIT) compiler. It has some limitations (doesn't
seem to work in the IPython console in PyCharm - maybe due to Django? But django setup in standalone scripts works). But
with some slight code modifications (it doesn't support `mode=same` on numpy convolutions and has some other quirks with core
Python code - doesn't like working with Django objects either (makes sense)). I'm still testing it right now, but I could see
a version of Mantis optimized for it, where needed values are copied out of django into standard Python values, and then
passed into functions that do the heavy lifting that only use core Python and numpy. That said, it seems like maybe the 
JIT might not be speeding up the convolution itself - in a test, the convolution took longer than previously, though that
test had to have a few different parameters (setting output into a new variable instead of back into existing array).
Still, seems like maybe it might not help the convolution. It possibly could still help with some of the matrix operations
that build the weight rasters, etc though.

As an alternative, Giorgos and I are trying to run Matlab compiled executables on the linux server. One option to make this
work sans-license is to use Octave in the meantime. It seems that the consensus is that Octave is generally a bit slower,
but it is probably acceptable for this prototype stage. It seems like the Matlab code might need some adaptation to work
with Octave still, from testing. 